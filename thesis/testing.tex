\chapter{Testing}
\minitoc

\new{
\section{Standardization in Space Industry}
Software development for Space, particularly in Europe, follows very specific rules on international standards. This standards not only define how a software project should
evolve but also how every component developed in the field of Space Industry should be done, tested, documented, deployed and maintained.
This include not only software but many more artefacts like: Electrical, Mechanical, Material components, etc.\\
In 1994 \ac{ECSS} was established to develop a single set of consistent Space standards recognized and accepted for use by the entire European Space Community.
\ac{ECSS} is supported by \ac{ESA} Council deliberations since the foundation and has developed through
a partnership between \ac{ESA}, National Space Agencies and European Industry, the management and implementation of European Space Projects.\\
\ac{ECSS} main objectives are increase the effectiveness of all Space programmes in Europe through the application of a
single, integrated set of Standards and Requirements from which all generic requirements of future Space projects can be derived and
improve the competitiveness of the European Space industry.
This is particularly important to formalize an unambiguous communication in order to facilitate the interaction between project partners,
create a set of legally binding documents, reduce risk and guarantee interface compatibility and
improve the quality and safety of Space projects and products.

\def\a{\save[].[dddddd]!C="a"*[F--]\frm{}\restore}
\def\b{\save[].[dddddddd]!C="b"*[F--]\frm{}\restore}

\xyoption{matrix}
\begin{figure}[!htb]\label{fig:ecssdocsstruct}
\footnotesize
\begin{displaymath}
\xymatrix{
  & *+[F-]{\txt{ECSS-S-ST-00\cite{ecss-s-st-00c} - ECSS System}} \ar@{-}[d] \ar@{-}[dl] \ar@{-}[dr] & \\
	\a{\txt{Space project management\\(management Standards)}}
  & \b{\txt{Space product assurance\\(product assurance Standards)}}
  & \b{\txt{Space engineering\\(engineering Standards)}}\\
	*+[F**:red]{\txt{M-10 - Project planning\\and implementation}}
  & *+[F**:blue][white]{\txt{Q-10 - Product\\assurance management}}
  & *+[F**:green]{\txt{E-10 - System\\engineering}}\\
	*+[F**:red]{\txt{M-40 - Configuration and\\information management}}
  & *+[F**:blue][white]{\txt{Q-20 - Quality assurance}}
  & *+[F**:green]{\txt{E-20 - Electrical and\\optical engineering}}\\
	*+[F**:red]{\txt{M-60 - Cost and schedule\\management}}
  & *+[F**:blue][white]{\txt{Q-30 - Dependability}}
  & *+[F**:green]{\txt{E-30 - Mechanical\\engineering}}\\
	*+[F**:red]{\txt{M-70 - Integrated logistic\\support}}
  & *+[F**:blue][white]{\txt{Q-40 - Safety}}
  & *+[F**:green]{\txt{E-40\cite{ecss-e-st-40c} - Software\\engineering}}\\
	*+[F**:red]{\txt{M-80 - Risk management}}
  & *+[F**:blue][white]{\txt{Q-60 - EEE components}}
  & *+[F**:green]{\txt{E-50 - Communications}}\\
  & *+[F**:blue][white]{\txt{Q-70 - Materials, mechanical\\parts and processes}}
  & *+[F**:green]{\txt{E-60 - Control\\engineering}}\\
  & *+[F**:blue][white]{\txt{Q-80\cite{ecss-q-st-80c} - Software\\product assurance}}
  & *+[F**:green]{\txt{E-70 - Ground systems\\and operations}}\\
  & & &
}
\end{displaymath}
    \caption{\protect\ac{ECSS} System}
\end{figure}

\xyoption{matrix}
\begin{figure}[!htb]\label{exp_struct_desenho}
\begin{displaymath}
\xymatrix{ \put(0,0){\circle*{3}} \ar[d]^{*proxL} \ar[r]^{*proxC} &
            \ldots \ar[r]^{*prox} &
            c_{0j} \ar[r]^{*prox} & NULL\\
          \put(0,0){\circle*{3}} \ar[d]^{*proxL} \ar[r]^{*proxC} &
            \ldots \ar[r]^{*prox} &
            c_{ij} \ar[r]^{*prox} & *+[F--]{NULL}\\
          NULL &\omit &\omit &\omit &\omit &\omit &\omit & \omit}
\end{displaymath}
    \caption{Explicacao da estrutura que guarda o desenho}
\end{figure}

%documneto que define o sistema ecss\cite{ecss-s-st-00c}
%documento de qualidade\cite{ecss-q-st-80c}
%ulisses paper\cite{costa_et_al:OASIcs:2012:3523}

\begin{description}
\item[Level A] Software that if not executed, or if not correctly executed, or whose anomalous behaviour can cause or contribute to a system failure resulting in \textbf{Catastrophic consequences};
\item[Level B] Software that if not executed, or if not correctly executed, or whose anomalous behaviour can cause or contribute to a system failure resulting in \textbf{Critical consequences};
\item[Level C] Software that if not executed, or if not correctly executed, or whose anomalous behaviour can cause or contribute to a system failure resulting in \textbf{Major consequences};
\item[Level D] Software that if not executed, or if not correctly executed, or whose anomalous behaviour can cause or contribute to a system failure resulting in \textbf{Minor or Negligible consequences}.
\end{description}

\begin{table}[!ht]
\centering
\noindent \begin{tabular}{|m{6cm}|c|c|c|c|}
\hline
\textbf{Code Coverage vs. Criticality} & A & B & C & D \\\hline
Source code statement coverage & 100\% & 100\% & AM & AM \\\hline
Source code decision coverage & 100\% & 100\% & AM & AM \\\hline
Source code modified condition and decision coverage & 100\% & AM & AM & AM \\\hline
\multicolumn{5}{|m{14cm}|}{
NOTE: "AM" means that the value is agreed with the customer and measured for:
unit level testing; integration level testing and validation against the
technical specification and validation against the requirements baseline
as in \cite{ecss-q-st-80c} at clause 6.3.5.2.
}\\\hline
\end{tabular}
\caption{\protect\ac{ECSS} - Code Coverage vs. Criticality}\label{tab:ccoverage}
\end{table}

\section{Code Coverage}
}
\note{falar mais sobre testing no geral, antes de comecar a falar de white vs black}
\section{White-box vs Black-box testing}
%In this section is discussed the two most common approaches for testing: White-box and Black-box testing.\\
In White-box testing the tester needs to understand the internals of
the code to be able to write tests for it.
The goal of selecting test cases that test specific parts of the code
is to cause the execution of specific spots in the software, such as
statements, branches or
paths.
This technique consists in analyzing statically a program, by reading
the program code and using symbolic execution techniques to simulate
abstract program
executions in order to attempt to compute inputs to drive the program
along specific execution paths or branches, without ever executing the
program. Control Flow based testing approach can be useful to analyze all the
possible paths in the code and write unit tests to cover multiple paths.
The \ac{CFG} of the program can be built,
test inputs can be generated to make any path execute regarding a given criterion:
Select all paths;
Select paths to achieve complete statement
coverage\cite{stt,Ntafos:1988:CST:630792.631017};
Select paths to achieve complete branch coverage\cite{Roper1994,stt};
or Select paths to achieve predicate
coverage\cite{stt,Ntafos:1988:CST:630792.631017}.

Data Flow Testing is designed into looking at the life cycle
(creation, usage and destruction) of a particular
piece of data and observe how it is used along the \ac{CFG}, this ensures
that the number of paths is always finite\cite{dataflow}.\\

Opposite to White-box testing, Black-box testing is based on
functionality, so the tester observes a system based
on its functional contracts and writes the pairs of inputs and the
expected outputs.
This approach is used for unit testing of single methods/functions,
integration testing
of combinations of the methods/functions, or even final system testing.\\

%This document is organized as follows.
%In Section~\ref{testingapproaches} the important testing approaches in
%use---Specification-based testing and Constraint-based generation---are briefly
%revisited and, for each one,  the most relevant tools are identified.
%In Section~\ref{testingtools} some of the tools referred are
%experimented in order to be compared.
%Our proposal for a test generation system is introduced in
%Section~\ref{proposal}.
%The document is concluded in Section~\ref{sec:Concl}.

\section{Testing Tools Approaches}\label{testingapproaches}
In this section, a study of the most recent tools that use Specification-based, Constraint-based, Grammar-based and Random-based tests generation
approaches for the most popular languages - C, JAVA and C\# will be presented.

\subsection{Specification-based Generation Testing}
Specification Based Testing refers to the process of testing a program based on what its specification or model says its behavior should be.
In particular, can be generated test cases based on the specification of the program's behavior, without seeing an implementation of the program. So this clearly a
way of Black-box testing.\\
With this technique the testing phase and development phase can be started in parallel, we do not need the implementation
to start the development of test cases. The only thing needed is the functional contracts and/or oracles\footnote{A test oracle determines whether or not the results of a test execution are correct\cite{Peters95generatinga}.} for each function/method.\\
Since the 90's there have been some effort into using specifications to try to generate test cases such as Z specifications
\cite{Horcher95improvingsoftware,Stocks:1996:FST:239916.239918}, \ac{UML} statecharts\cite{Offutt:1999:GTU:1767297.1767341}, \ac{VDM}\cite{Aichernig99automatedblack-box}
or \ac{ADL} specifications\cite{Sankar94specifyingand}.
These specifications typically do not consider structurally complex inputs and these tools do not generate JUnit test cases.
Nowadays there are some tools out there that can perform Specification-based Testing approach:

\begin{description}
\item[Conformiq] is a commercial Tool Suite that generates
human-readable test plans and executable test scripts from Java code, state charts and \ac{UML}\footnote{See more at: \url{http://www.conformiq.com/products.php}}.
\item[MaTeLo] stands for Markov Test Logic and is a commercial tool
that generates test sequences from a collection of states, transitions, classes of equivalence, types, sequences, global variables and test oracles
using their user interface\footnote{See more at: \url{http://www.all4tec.net/index.php/All4tec/matelo-product.html}}.
\item[Smartesting CertifyIt] is a commercial tool that generates test cases from a functional model, as \ac{UML}\footnote{See more at: \url{http://www.smartesting.com/index.php/cms/en/product/certify-it}}.
\item[T-Vec] is a commercial tool that generates test cases from modeling tools available from T-VEC or third-party vendors\footnote{See more at: \url{http://www.t-vec.com/}}.
\item[Rational Tau] is an \ac{IBM} commercial tool that provides automated error checking, rules-based model checking, and a model-based explorer using
\ac{UML}\footnote{See more at: \url{http://www-01.ibm.com/software/awdtools/tau/}}.
\end{description}
The relevant ones or the recent open-source ones will be discussed.

\subsubsection{Spec Explorer}
This is a Microsoft model-based testing that uses one software modeling languages, the \ac{AsmL}.
This modeling language provides the foundations of the Spec Explorer\footnote{See more at: \url{http://research.microsoft.com/en-us/projects/specexplorer/}} tool
and Spec\# that is a formal language for \ac{API} contracts (influenced by \ac{JML}, \ac{AsmL}, and Eiffel), which extends C\# with constructs for non-null types,
pre-conditions, post-conditions, and object invariants\footnote{See more at: \url{http://research.microsoft.com/en-us/projects/specsharp/}}.
These tool is already available to users and is in a very mature phase.\\
\indent The user of Spec Explorer writes a model of the system and sets the possible values for some properties in his code, furthermore the user also provides a scenario.
These scenarios are simple sets of calls to methods without their parameters (remember that this is Spec Explorer job).
Then Spec Explorer will generate a visual graph where each node represents a state of the system and the arrows represent a call to some method.
It searches throw all possible sequences of methods invocation that do not violate the contracts (pre, pos conditions) and
that are relevant to a user-specified set of test properties. After that we can generate from this visual graphs the unit tests (the arrows) and the
test cases (a graph).

\subsubsection{JMLUnit}
JMLUnit\cite{Cheon04thejml} is a tool that automates the generation of oracles for JAVA testing classes. This tool
monitors the specified behavior of the method being tested to decide whether the test passed or failed.
This monitoring is done using the formal specification language runtime assertion checker.
The main idea behind these tools is to translate the pre- and post-conditions methods into the code of the testing method.\\
The pre-conditions became the criteria for selecting test inputs, and the post-conditions provided the properties to check for
test results. So, the post-conditions became the test oracles.\\
This tool uses the \ac{JML}\cite{Burdy03anoverview} specification language to annotate JAVA methods code with pre- and post-conditions and
automatically generate JUnit test classes from \ac{JML} specifications.

\subsubsection{TestEra}
TestEra\cite{testera} can be used to perform automated specification-based testing of
JAVA programs. This framework requires as input a JAVA method, a formal specification\footnote{Specifications are first-order logic formulae.}
of the pre and post-conditions of that method, and a bound that limits the size of the test cases to be generated.\\
With the pre-condition it automatically generates all non-isomorphic test inputs up to the given bound.
It executes the method on each test input, and uses the method post-condition as an oracle to check the correctness of each output. This tool
uses Alloy's\footnote{Alloy is a first-order declarative language based on sets and relations. The Alloy Analyzer is a fully
automatic tool that finds instances of Alloy specifications: an instance
assigns values to the sets and relations in the specification such that
all formulae in the specification evaluate to true.} \ac{SAT} system to analyze first-order  formulae.
The authors claim that have used TestEra to check several JAVA programs including an architecture for
dynamic networks, the Alloy-alpha analyzer, a fault-tree analyzer, and methods from the JAVA Collection Framework.

\subsubsection{Korat}
Korat\cite{Boyapati02korat:automated} is a mature framework for automated testing structurally complex inputs of JAVA programs.
Given a formal specification for a method, Korat\footnote{See more at: \url{http://korat.sourceforge.net/}} uses the method pre-condition
to automatically generate all (non-isomorphic) test cases up to a given small size.
Korat then executes the method on each test case, and uses the method post-condition as a test oracle to check the correctness of each output.\\
To be able to generate test cases for a method, Korat uses a predicate and a bound on the size of its inputs,
Korat generates all (non-isomorphic) inputs for which the predicate returns $true$.
Korat generates all the possible input spaces regarding the predicate and monitor the predicate's executions to be able to prune large portions of the search space.\\
\indent The writing of a predicate is done using JAVA language and in most cases can be written the first thing that cames to programmer's head to restrict the input space.
But for more complex structures it is better to understand how the matching algorithm work to be able to write a fast verifiable predicate.\\
Unfortunately the test derivation tool using Korat (that also uses \ac{JML}) is not available to the public.

\subsection{Constraint-based Generation Testing}
Constraint Based Testing\cite{DeMillo91constraint-basedautomatic} can be used to select test cases satisfying specific constraints by
solving a set of constraints over a set of variables. The system is described using constraints and these can be solved by \ac{SAT} solvers.\\
Constraint programming can be combined with symbolic execution, regarding this approach a program is executed symbolically,
collecting data constraints over different paths in the \ac{CFG}, and then solving the constraints and producing test cases from there.
There are some tools out there, like:

\begin{description}
\item[Euclide] for verifying safety properties over C code using \ac{ACSL} annotations, CPBPV for program verification.
\item[OSMOSE] a tool that uses concolic execution and path-based techniques over machine code.
\item[GATeL] for Lustre language to generate test sequences\footnote{See more at: \url{http://www-list.cea.fr/labos/gb/LSL/test/gatel/index.html}}.
\end{description}

Here two tools will be explained, one proprietary and other academic.

\subsubsection{Pex} Pex\cite{Tillmann:2008:PWB:1792786.1792798} is an automatic white-box test generation tool for .NET. Starting from a
method that takes parameters, Pex performs path-bounded model-checking
by repeatedly executing the program and solving constraint systems to obtain inputs that will steer the program along different execution paths.
This uses the idea of dynamic symbolic execution\cite{Tillmann06unittests}. Pex uses the theorem prover and
constraint solver Z3\footnote{See more at: \url{http://research.microsoft.com/en-us/um/redmond/projects/z3/}} to reason about the feasibility of execution paths, and
to obtain ground models for constraint systems.\\
Pex came with Moles that helps to generate unit tests. These tools together are able to understand the input (by analyzing branches in the code:
declarations, all exceptions throws operations, if statements, asserts and .net Contracts). With this information Pex uses Z3 constraint solver to
produce new test inputs which exercise diferent program behavior.\\
The result is an automatically generated small test suite which often achieves high code coverage.\\
Pex can be used in a project, class or method (which makes it a very helpful and versatile tool). After the analysis process the "Pex Explorarion Results" shows
the $input \times output$ pairs selected for each test case for the method, here it also shows the percentage of the test coverage.

\subsubsection{PathCrawler} This is an academic tool based on dynamic and static analysis\cite{Williams05pathcrawler:automatic}, 
it uses constraint logic programming to generate the Test-cases. PathCrawler\footnote{See more at: \url{http://www-list.cea.fr/labos/gb/LSL/test/pathcrawler/index.html}} executes an instrumented function for each function under test
with the generated inputs, it preserves this information to not cover the same path.\\
This tool supports assertions in any point in the code and pre-conditions regarding the input values.

\subsection{Grammar-based Generation Testing}
In this approach inputs to a system under test are defined by a context-free grammar. The language of the grammar contains all possible test cases.
Using this approach to describe the syntax of the input to the system under test proves to be very helpful to test
network protocols\cite{tal:syntax-based,kaksonen2001functional} and parsers and compilers\cite{1994-burgess,Burgess_Saidi_1996}.

\subsubsection{ASTGen}
ASTGen\cite{Daniel:2007:ATR:1287624.1287651} is a JAVA framework that automates testing of refactoring engines: generation of test inputs
and checking of test outputs. The main technique is an iterative generation of structurally complex test inputs.
ASTGen\footnote{See more at: \url{http://mir.cs.illinois.edu/astgen/}} allows developers to write imperative generators whose executions
produce input programs for refactoring engines. More precisely, ASTGen
offers a library of generic, reusable, and composable generators that produce \ac{AST}.\\
So, ASTGen ensures the production of test inputs instead of the developer produce them. The developer needs to write a generator whose execution
produces thousands of programs with structural properties that are relevant for the specific refactoring being tested. This tool has found
21 bugs in Eclipse and 26 bugs in Netbeans applications.

\subsection{Random-based Generation Testing}
In the random testing approach, test inputs are selected randomly from the input domain of the system.
To have a random testing suite first we must identify the input domain, after that select test inputs independently from the domain,
then the system under test is executed on these inputs, the results are compared to the system specification, an oracle.\\
Random testing gives us an advantage of easily estimating software reliability from test outcomes.
Test inputs are randomly generated according to an operational profile, and failure times are recorded.
The data obtained from random testing can then be used to find bugs or non expected behaviors.\\
\indent The main problem regarding random generation is the problem of the coverage, it is possible that it will not be broad enough. And furthermore it can be
too sparse to actually test specifics parts of the program. Either way, this technique proves to be very effective for testing compilers.

\subsubsection{Csmith}
Csmith\cite{Yang:2011:FUB:1993316.1993532} is a black-box random tests generator that is able to generate C programs
conform to the C99\footnote{See more at: \url{http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1256.pdf}} standard. This is a very recent tool that already discover
more than 195 bugs in \ac{LLVM} and 79 bugs in \ac{GCC}. With Csmith we are able to generate random programs with unambiguous meanings (undefined behavior or 
unspecified behavior). Does not attempt to generate terminating program, so they use timeouts for long time consuming generated programs.
And the main supported features right now are: Arithmetic, logical, and bit operations on integers, Loops, Conditionals, Function calls, Const and volatile,
Structs and Bitfields, Pointers and arrays, Goto, Break and continue. The generation of code regarding this features can be tuned using the command line program.

\subsubsection{QuickCheck for JAVA}
QuickCheck was originally a combinator library for the Haskell\footnote{See more at haskell.org} programming language\cite{Claessen:2000:QLT:357766.351266}.
Later on QuickCheck philosophy spread to other programming languages like: JAVA, Erlang, Perl, Ruby and JavaScript.\\
QuickCheck works by generating high amounts of data (within the method domain) and checking it against a given property,
it is expected to create a wide range of the input domain, thus increasing the chances of giving more test coverage.
\secendnote
